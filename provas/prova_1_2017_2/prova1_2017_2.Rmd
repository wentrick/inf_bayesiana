---
title: "Prova 1-2017/2"
author: "Davi Wentrick Feijó"
date: "2023-10-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Pergunta 1

Seja uma amostra \(X\) da distribuição exponencial com parâmetro \(\theta\) e densidade \(p(x|\theta) = \theta e^{-\theta x}\) (\(x, \theta > 0\)) e considere uma distribuição a priori \(\theta \sim \text{Gama}(\alpha, \beta)\).

#### (a) Calcule a distribuição a posteriori \(p(\theta | X)\).

Seja $X = (X_1,...,X_n)$

Sabemos que para calcular a posteriori podemos usar a seguinte formula:


$$
p(\theta | X) = \frac{p(X | \theta) \cdot p(\theta)}{p(X)} 
$$

Sabemos que :

+ $p(X | \theta)$ é a nossa verossimilhanca 

+ $p(\theta)$ é a priori

+ $p(X)$ é a constante de normalizacao 

Entretanto a posteriori pode ser expressa da seguinte forma:

$$
p(\theta | X)  \varpropto  p(X | \theta) \cdot p(\theta) 
$$

Isso significa que a distribuicao a posteriori vai ser proporcional a $p(X | \theta) \cdot p(\theta)$. O que faz sentido tendo em vista que tiramos a constante de normalizacao, logo nao vao estar na mesma escala e por isso serao proporcionais.

O problema nos da:

+ $p(X | \theta)$   = $\theta e^{-\theta x}$

+ $p(\theta)$ = $\text{Gama}(\alpha, \beta)$ = $\frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{\alpha - 1} e^{-\beta \theta}$


Vamos encontrar $p(\theta | X)$ nossa verossimilhanca:

$$
p(X | \theta) = \prod_{i=1}^{n}(\theta e^{-\theta x_i}) = \theta^n e^{-\theta n \bar X}
$$
Logo podemos ir para o calculo da posteriori:

$$

\begin{align}

p(\theta | X)  &\varpropto  p(X | \theta) \cdot p(\theta) \\

&\varpropto  \theta^n e^{-\theta n \bar X} \cdot \text{Gama}(\alpha, \beta) \\

&\varpropto  \theta^n e^{-\theta n \bar X} \cdot \frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{\alpha - 1} e^{-\beta \theta} \\

&\varpropto  \theta^n e^{-\theta n \bar X} \cdot  \theta^{\alpha - 1} e^{-\beta \theta} \\

&\varpropto  \theta^{(n+\alpha)-1} e^{-(\beta+ n \bar X) \theta} \sim Gama(n+\alpha,\beta+ n \bar X) \\

\end{align}

$$

Obs: Foi retirado os termos constantes que nao dependem de \theta devido ao fato de estarmos buscando uma distribuicao proporcional, esses termos seriam para normalizacao da distribuicao econtrada. Ao final da conta foi feita manipulacao nos expoentes para que fique no mesmo formato de uma distribuicao Gama


#### (b) Calcule o valor esperado a posteriori de \(\theta\) e mostre que ele pode ser escrito da forma \(wE(\theta) + (1 - w)\hat{\theta}\), onde \(E(\theta)\) é o valor esperado a priori e \(\hat{\theta}\) é o estimador de máxima verossimilhança.

Sabemos que a distribuicao Gama tem esperanca na forma : $\frac{\alpha}{\beta}$, logo o valor esperado a posteriori será:

$$
E(\theta)_{post} = \frac{n+\alpha}{\beta+ n \bar X}
$$

Para a realizacao do calculo vamos tomar a reciproca da $E(\theta)_{post}$:

$$
\begin{align}

E(\theta)_{post}^{-1} &= \frac{ n \bar X+\beta}{\alpha+n} \\

 &= \frac{ n \bar X+\beta}{\alpha+n} \\
 &= \frac{ \sum_{i=1}^{n}x_i+\beta}{\alpha+n} \\
 &= \frac{ \sum_{i=1}^{n}x_i}{\alpha+n} \cdot \frac{n}{n} + \frac{\beta}{\alpha+n} \cdot \frac{\alpha}{\alpha} \\
 &= \frac{n \sum_{i=1}^{n}x_i }{(\alpha+n)n}  + \frac{\beta \alpha}{(\alpha+n) \alpha} \\
 &= \frac{ n}{\alpha+n} \cdot \frac{\sum_{i=1}^{n}x_i}{n} + \frac{\alpha}{\alpha+n} \cdot \frac{\beta}{\alpha} \\
\end{align}
$$

Apos as manipulacoes algebricas, chegamos numa forma que é similar a forma desejada $wE(\theta) + (1 - w)\hat{\theta}$:

+ $E(\theta)$ = $\frac{\sum_{i=1}^{n}x_i}{n}$

+ $\hat{\theta}$ = $\frac{\beta}{\alpha}$

Nosso $w$ será:

$$
w = \frac{n}{\alpha+n} 
$$
E a partir disso podemos calcular $1-w$

$$
 1-w = 1 - \frac{n}{\alpha+n} = \frac{\alpha+n-n}{\alpha+n} = \frac{\alpha}{\alpha+n}
$$

Mostramos que os resultados batem com o esperado, logo a equacao pode ser reescrita da seguinte forma:

+ $E(\theta)$ = $\frac{\sum_{i=1}^{n}x_i}{n}$

+ $\hat{\theta}$ = $\frac{\beta}{\alpha}$

+ $W$ = $\frac{n}{\alpha+n}$

+ $1-W$ = $\frac{\alpha}{\alpha+n}$

$$
\frac{ n}{\alpha+n} \cdot \frac{\sum_{i=1}^{n}x_i}{n} + \frac{\alpha}{\alpha+n} \cdot \frac{\beta}{\alpha} = wE(\theta) + (1 - w)\hat{\theta}
$$


#### (c) Se \(X_{n+1}\) é uma observação futura da mesma distribuição exponencial, calcule a distribuição preditiva \(p(x_{n+1} | X)\) e o valor esperado \(E(X_{n+1} | X)\).

Seja $X = (X_1,...,X_n)$

Sabemos que:

$$
p(\theta | X) \varpropto \theta^{(n+\alpha)-1} e^{-(\beta+ n \bar X) \theta} \sim Gama(n+\alpha,\beta+ n \bar X)
$$

Os hiperparametros da distribuicao a posteriori é:

$$
\alpha^{\ast} = n+\alpha \ \ \  \ \ \ \beta^{\ast} = \beta+ n \bar X \ \ \ \sim Gama(\alpha^{\ast},\beta^{\ast}) 
$$
Vamos calcular a nossa $p(x_{n+1} | X)$

$$
p(x_{n+1} | X) = \theta^1e^{-\theta x}
$$
Como estamos obersvando 1 nova observacao entao é encessario somente ver como ficaria o calculo de uma distribuicao exponencia com n=1


$$
\begin{align}
p(x_{n+1}|X) &= \int p(\theta|X) \cdot p(X|\theta) d\theta \\
&= \int \frac{\beta^{\ast\alpha^{\ast}}}{\Gamma(\alpha^{\ast})} \theta^{\alpha^{\ast} - 1} e^{-\beta^{\ast} \theta} \cdot \theta^1e^{-\theta x} d\theta \\
&= \int \frac{\beta^{\ast\alpha^{\ast}}}{\Gamma(\alpha^{\ast})} \theta^{(\alpha^{\ast} + 1) - 1} e^{-\beta^{\ast} \theta -\theta x}  d\theta \\
&= \int \frac{\beta^{\ast\alpha^{\ast}}}{\Gamma(\alpha^{\ast})} \theta^{(\alpha^{\ast} + 1) - 1} e^{-\theta(\beta^{\ast} + x)}  d\theta \\
&= \int \frac{\beta^{\ast\alpha^{\ast}}}{\Gamma(\alpha^{\ast})} \theta^{(\alpha^{\ast} + 1) - 1} e^{-\theta(\beta^{\ast} + x)}  d\theta \\
\end{align}
$$
Agora temos que manipular essa integral para dar 1, inicialmente vamos chamar de $\alpha^{\prime} = \alpha^{\ast} + 1$ $\beta^{\prime} = \beta^{\ast} + x$ e logo poderemos perceber que isso parece uma $Gama(\alpha^{\prime},\beta^{\prime})$ contudo a constante normalizadora ($\frac{\beta^{\ast\alpha^{\ast}}}{\Gamma(\alpha^{\ast})}$) nao esta certa, para isso vamos ter que fazer algumas mudancas

$$
\begin{align}
&= \int \frac{\beta^{\ast\alpha^{\ast}}}{\Gamma(\alpha^{\ast})} \theta^{\alpha^{\prime} - 1} e^{-\theta\beta^{\prime}}  d\theta \\
&= \frac{\Gamma(\alpha^{\prime})}{\Gamma(\alpha^{\prime})} \frac{\beta^{\prime\alpha^{\prime}}}{\beta^{\prime\alpha^{\prime}}} \int \frac{\beta^{\ast\alpha^{\ast}}}{\Gamma(\alpha^{\ast})} \theta^{\alpha^{\prime} - 1} e^{-\theta\beta^{\prime}}  d\theta \\
&= \frac{\Gamma(\alpha^{\prime})}{\Gamma(\alpha^{\prime})} \frac{\beta^{\prime\alpha^{\prime}}}{\beta^{\prime\alpha^{\prime}}} \frac{\beta^{\ast\alpha^{\ast}}}{\Gamma(\alpha^{\ast})} \int  \theta^{\alpha^{\prime} - 1} e^{-\theta\beta^{\prime}}  d\theta \\
&= \frac{\Gamma(\alpha^{\prime})}{\beta^{\prime\alpha^{\prime}}}  \frac{\beta^{\ast\alpha^{\ast}}}{\Gamma(\alpha^{\ast})} \int \frac{\beta^{\prime\alpha^{\prime}}}{\Gamma(\alpha^{\prime})} \theta^{\alpha^{\prime} - 1} e^{-\theta\beta^{\prime}}  d\theta \\
\end{align}
$$

Agora sabemos que essa integral segue $Gama(\alpha^{\prime},\beta^{\prime})$ logo o resultado dela sera 1


$$
\begin{align}
&= \frac{\Gamma(\alpha^{\prime})}{\Gamma(\alpha^{\ast})}  \frac{\beta^{\ast\alpha^{\ast}}}{\beta^{\prime\alpha^{\prime}}} \int \frac{\beta^{\prime\alpha^{\prime}}}{\Gamma(\alpha^{\prime})} \theta^{\alpha^{\prime} - 1} e^{-\theta\beta^{\prime}}  d\theta \\

&= \frac{\Gamma(\alpha^{\ast} + 1)}{\Gamma(\alpha^{\ast})}  \frac{\beta^{\ast\alpha^{\ast}}}{\beta^{\prime\alpha^{\prime}}} \\

&= \frac{\alpha^{\ast}!}{(\alpha^{\ast}-1)!}  \frac{\beta^{\ast\alpha^{\ast}}}{\beta^{\prime\alpha^{\prime}}} \\

&= \frac{\alpha^{\ast}(\alpha^{\ast}-1)!}{(\alpha^{\ast}-1)!}   \frac{\beta^{\ast\alpha^{\ast}}}{\beta^{\prime\alpha^{\prime}}} \\

&=  \frac{\alpha^{\ast}\beta^{\ast\alpha^{\ast}}}{\beta^{\prime\alpha^{\prime}}} \varpropto \frac{1}{\beta^{\prime\alpha^{\prime}}} \\
\end{align}
$$

Obs: $\Gamma(\alpha^{\ast} + 1) = \alpha^{\ast}!$ assim como $\Gamma(\alpha^{\ast}) =  (\alpha^{\ast}-1)!$ sao propriedades da Funcao Gama. Alem disso esse resultado final é necessario ter uma interpretacao apra entende-lo. tendo em vista que $\alpha^{\ast}$ e $\beta^{\ast}$ vinheram do nosso calculo da posteriori, ou seja nesse caso ja conhecemos eles, contudo o que é novo devido a nova observacao é $\alpha^{\prime}$ e $\beta^{\prime}$ com isso podemos usar a proporcionalidade para retirar o que ja sabemos (ja foi observado) para aquilo que ainda nao foi que é a distribuicao que queremos encontrar.


$$
\frac{1}{\beta^{\prime\alpha^{\prime}}} = (\frac{1}{\beta^{\prime\alpha^{\prime}}})^{-1} = \beta^{\prime-\alpha^{\prime}} = (\beta^{\ast} + x)^{-(\alpha^{\ast} + 1)} \sim \text{Pareto Tipo II}
$$
A esperanca pode ser calculada :

$$
E(p(x_{n+1}|X)) = \frac{\beta^{\ast}}{(\alpha^{\ast} + 1)-1} = \frac{\beta+ n \bar X}{n+\alpha}
$$



### Pergunta 2

Sejam \(y_1, y_2, \ldots, y_n\) observações independentes com \(y_i \sim \text{Poisson}(\theta x_i)\), onde as exposições \(x_i\) são fixas (não aleatórias). A taxa \(\theta\) segue a priori uma distribuição \(\text{Gama}(\alpha, \beta)\).

#### (a) Ache a distribuição a posteriori de \(\theta\) e sua média e variância.

Sabemos que :

$$
y_i|\theta \sim \text{Poisson}(\theta x_i)
$$
Ou seja cada observacao segue uma poisson, entretanto para resolver precisamos encontrar a distribuicao de Y. Para isso vamos realizar o somatorio em $y_i$.

$$
Y = \sum_{i=1}^{n} y_i|\theta \sim \text{Poisson}(\theta n \bar X) = \frac{(\theta n \bar X)^{y_i}e^{-n \bar X \theta}}{y_i!} 
$$

Obs:  A substituicao feita foi com base na formula para o calculo da media onde $\sum_{i=1}^{n} x_i = n \bar X$


Agora podemos calcular a verossimilhanca:

$$
\begin{align}
p(Y|n \bar X) &= \prod_{i=1}^{n} \frac{(\theta n \bar X)^{y_i}e^{-n \bar X \theta}}{y_i!}  \\
 &=  \frac{(\theta n \bar X)^{n \bar Y}e^{-n \bar X \theta}}{y_i!} \varpropto  \theta^{n \bar Y}  e^{-n \bar X \theta}
\end{align}
$$
Foram retirados todos as partes que sao independentes de \theta por isso usamos o simbolo de proporcionalidade ja que sao termos responsaveis pela normalizacao da distribuicao (entre 0 e 1).

A nossa priori é:

$$
p(\theta) \sim Gama(\alpha,\beta) \\
$$
$$
p(\theta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{\alpha - 1} e^{-\beta \theta} \\
p(\theta) \varpropto  \theta^{\alpha - 1} e^{-\beta \theta}
$$

Podemos iniciar o calculo da posteriori:

$$
\begin{align}
p(\theta|Y) \varpropto p(Y|n \bar X) \cdot p(\theta) &=  \theta^{n \bar Y}  e^{-n \bar X \theta} \cdot \theta^{\alpha - 1} e^{-\beta \theta}\\
&=  \theta^{(n \bar Y+\alpha)-1}  e^{-(\beta + n \bar X) \theta} \sim Gama(n \bar Y+\alpha,\beta + n \bar X)
\end{align}
$$

Com a distribuicao calculada podemos encontrar a média e a variancia com base nas formulas da Gama, logo:

$$
E(\theta|Y) = \frac{n \bar Y+\alpha}{\beta + n \bar X} \ \ \ \ \ \  \ \  \ Var(\theta|Y) = \frac{n \bar Y+\alpha}{(\beta + n \bar X)^2}
$$


#### (b) Mostre que é possível expressar a esperança a posteriori de \(\theta\) da forma \((1 - w)E(\theta) + w\hat{\theta}\), onde \(E(\theta)\) e \(\hat{\theta}\) são, respectivamente, a esperança a priori e a estimativa de máxima verossimilhança de \(\theta\), e interprete este resultado.

Vamos encontrar o $\hat \theta$

$$
\begin{align}
\hat \theta &= \frac{\partial }{\partial \theta} log(\theta^{n \bar Y}  e^{-n \bar X \theta}) = 0 \\ 
&= \frac{\partial }{\partial \theta} n \bar Ylog(\theta)-n \bar X \theta = 0 \\
&=  \frac{n \bar Y}{\hat\theta}-n \bar X  = 0 \\
&=  n[\frac{ \bar Y}{\hat\theta}- \bar X]  = 0 \\
&=  \frac{ \bar Y}{\hat\theta}- \bar X  = \frac{0}{n} \\
&=  \frac{ \bar Y}{\hat\theta}- \bar X  = 0 \\
&=  \frac{ \bar Y}{\hat\theta}   = \bar X \\
&=  \frac{ \bar Y}{\bar X}   =  \hat\theta\\
\end{align}
$$
$E(\theta)$ é a nossa esperanca a priora ou seja é a esperanca que vem da distribuicao $Gama(\alpha,\beta)$

$$
E(\theta) = \frac{\alpha}{\beta}
$$


Com isso podemos ir para a prova:

$$
\begin{align}
E(\theta|Y) &=  \frac{n \bar Y+\alpha}{\beta + n \bar X} \\
&=  \frac{n \bar Y}{\beta + n \bar X} + \frac{\alpha}{\beta + n \bar X}\\
&=  \frac{n \bar Y}{\beta + n \bar X} \cdot \frac{\bar X}{\bar X} + \frac{\alpha}{\beta + n \bar X} \cdot \frac{\beta}{\beta} \\
&=  \frac{n \bar X}{\beta + n \bar X} \cdot \frac{\bar Y}{\bar X} + \frac{\beta}{\beta + n \bar X} \cdot \frac{\alpha}{\beta} \\
\end{align}
$$

Agora que manipulamos a equacao para ficar no formato desejado temos:

+ $w = \frac{n \bar X}{\beta + n \bar X}$

+ $1 - w = \frac{\beta}{\beta + n \bar X}$

+ $E(\theta) = \frac{\alpha}{\beta}$

+ $\hat\theta = \frac{ \bar Y}{\bar X}$

Ou seja:

$$
\frac{n \bar X}{\beta + n \bar X} \cdot \frac{\bar Y}{\bar X} + \frac{\beta}{\beta + n \bar X} \cdot \frac{\alpha}{\beta} = (1 - w)E(\theta) + w\hat{\theta}
$$


#### (c) O que acontece na parte (b) quando \(\beta\) é grande com \(\alpha\beta^{-1}\) fixo? Interprete!

$$
\lim_{\beta\rightarrow \infty} \frac{n \bar X}{\beta + n \bar X} \cdot \frac{\bar Y}{\bar X} + \frac{\beta}{\beta + n \bar X} \cdot \frac{\alpha}{\beta} \\
\lim_{\beta\rightarrow \infty} 0 \cdot \frac{\bar Y}{\bar X} + 1 \cdot \frac{\alpha}{\beta}  =  \frac{\alpha}{\beta}
$$


### Pergunta 3

Seja \(y_1, y_2, \ldots, y_n\) uma amostra da distribuição de Bernoulli com probabilidade de sucesso \(\theta\) e considere uma distribuição a priori uniforme para \(\theta\).

#### (a) Ache a distribuição a posteriori de \(\theta\) e sua média e variância.

$$
Y = \sum y_i = Binomial(n,\theta)
$$


Sabemos que para calcular a posteriori podemos usar a seguinte formula:


$$
p(\theta | X) = \frac{p(X | \theta) \cdot p(\theta)}{p(X)} 
$$

Sabemos que :

+ $p(X | \theta)$ é a nossa verossimilhanca 

+ $p(\theta)$ é a priori

+ $p(X)$ é a constante de normalizacao 

Entretanto a posteriori pode ser expressa da seguinte forma:

$$
p(\theta | X)  \varpropto  p(X | \theta) \cdot p(\theta) 
$$

Isso significa que a distribuicao a posteriori vai ser proporcional a $p(X | \theta) \cdot p(\theta)$. O que faz sentido tendo em vista que tiramos a constante de normalizacao, logo nao vao estar na mesma escala e por isso serao proporcionais.

O problema nos da:

+ $p(X | \theta)$ = ${n \brack x} \theta^x(1-\theta)^{n-x}$

+ $p(\theta)$ = $\frac{1}{b-a}$


Vamos encontrar $p(\theta | X)$ nossa verossimilhanca:

Sabemos que a distribuicao binomial é uma serie de Bernoulli por isso usamos ela no calculos da verossimilhanca

$$
p(Y | \theta) = \prod_{i=1}^{n}(\theta^{y_i}(1-\theta)^{1-y_i}) = \theta^{\sum y_i}(1-\theta)^{n-\sum y_i} 
$$

A nossa priori(\theta) segue uma $\text{Uniforme}(0,1)$ porem ela pode ser representada por uma $\text{Beta}(1,1)$ por isso no calculo da posteriori vamos usar a $\text{Beta}(a,b)$

$$
\text{Beta}(a,b) = \theta^{a-1}(1-\theta)^{b-1} \\

E(X) = \frac{\alpha}{\alpha + \beta} \ \ \  \ \ Var(X) = \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha+\beta+1)}
$$

$$
z = \sum y_i
$$

$$

\begin{align}

p(\theta | Y)  &\varpropto  p(Y | \theta) \cdot p(\theta) \\

&\varpropto  \theta^{z}(1-\theta)^{n-z} \cdot \text{Beta}(a,b) \\

&\varpropto  \theta^{z}(1-\theta)^{n-z} \cdot \theta^{a-1}(1-\theta)^{b-1} \\

&\varpropto  \theta^{z+(a-1)}(1-\theta)^{(n-z)+(b-1)} \\

&\varpropto  \theta^{(z+a)-1}(1-\theta)^{(n-z+b)-1} \sim Beta(z+a,n-z+b) \\

\end{align}

$$


$$
E(\theta | z) = \frac{z+a}{(z+a) + (n-z+b)}  = \frac{z+a}{a +z+b}  \ \ \ \ \ Var(\theta | z) =\frac{z+a n-z+b}{(z+a + n-z+b)^2(\alpha+n-z+b+1)} = \frac{a+n+b}{(a+n+b)^2(a+n+b+1)}
$$

#### (b) Mostre que é possível expressar a esperança a posteriori de \(\theta\) da forma \((1 - w)E(\theta) + w\hat{\theta}\), onde \(E(\theta)\) e \(\hat{\theta}\) são, respectivamente, a esperança a priori e a estimativa de máxima verossimilhança de \(\theta\), e interprete este resultado.

Vamos encontrar o $\hat \theta$

$$
\begin{align}
\hat \theta &= \frac{\partial }{\partial \theta} log(\theta^{z}(1-\theta)^{n-z}) = 0 \\ 
&= \frac{\partial }{\partial \theta} z \cdot log(\theta) + (n-z) \cdot log(1-\theta) = 0 \\ 
&= \frac{z}{\hat \theta}  - \frac{ (n-z)}{1- \hat \theta}   = 0 \\ 
&= \frac{z}{\hat \theta} = \frac{ (n-z)}{1- \hat \theta} \\ 
&= \frac{1- \hat \theta}{\hat \theta} = \frac{ (n-z)}{z} \\ 
&= \frac{1}{\hat \theta} - 1 = \frac{ n}{z} - 1 \\
&= \frac{1}{\hat \theta}  = \frac{ n}{z}  \\
&= \hat \theta  = \frac{z}{n}  = \bar Y \\

\end{align}
$$

$E(\theta)$ é a nossa esperanca a priora ou seja é a esperanca que vem da distribuicao $Beta(a,b)$

$$
E(\theta) = \frac{\alpha}{\alpha + \beta}
$$

Vamos para a prova:

$$
\begin{align}
\theta_{post} &= \frac{a+z}{a+b+n} \\
&= \frac{a}{a+b+n}+\frac{z}{a+b+n} \\
&= \frac{a}{a+b+n}\cdot \frac{(a+b)}{(a+b)}+\frac{z}{a+b+n} \cdot \frac{n}{n} \\
&= \frac{(a+b)}{a+b+n}\cdot \frac{a}{(a+b)}+\frac{n}{a+b+n} \cdot \frac{z}{n} \\
&= \frac{(a+b)}{a+b+n}\cdot E(\theta)+\frac{n}{a+b+n} \cdot \hat \theta \\
\end{align}
$$

+ $w = \frac{n}{a+b+n}$

+ $1-w =  \frac{(a+b)}{a+b+n}$

#### (c) Se \(y_{n+1}\) é uma observação futura deste processo de Bernoulli, ache a distribuição preditiva \(p(y_{n+1}|y_1, \ldots, y_n)\).

$$
T = x_{n+1} \sim \text{Bernoulli}(\theta)
$$
Nos queremos encontrar $p(T|y)$ que vai ser nossa distribuicao preditiva para isso vamos ter que encontra-lá a aprtir da nossa distribuicao conjunta:

$$
\begin{align}
p(\theta,y,T) &= p(\theta) \cdot p(y|\theta) \cdot p(T|\theta) \\
p(\theta,T|y)  p(y) &= p(\theta) \cdot p(y|\theta) \cdot p(T|\theta) \\
p(\theta,T|y)  &= \frac{p(\theta) \cdot p(y|\theta)}{p(y)} \cdot p(T|\theta) \\
p(\theta,T|y)  &= p(\theta|y) \cdot p(T|\theta) \\
\end{align}
$$

Agora que fizemos as devidas manipulacoes e encontramos a posteriori, podemos marginalizar essa condicional, ou seja integrar em relacao a \theta que com isso encontramos a distribuicao preditiva:

$$
\begin{align}
p(T|y) &= \int_{\theta}p(\theta,T|y)d\theta  = \int_{\theta}  p(\theta|y) \cdot p(T|\theta) d\theta \\
p(T|y) &= \int_{\theta}  p(\theta|y) \cdot p(T|\theta) d\theta \\
p(T|y) &= \int_{\theta}  Beta(z+a,n-z+b) \cdot Bernoulli(\theta) d\theta\\
p(T|y) &= \int_{\theta} \frac{1}{Beta(z+a,n-z+b)} \theta^{(z+a)-1}(1-\theta)^{(n-z+b)-1} \cdot \theta^{T}(1-\theta)^{1-T}  d\theta\\
p(T|y) &= \int_{\theta} \frac{1}{Beta(z+a,n-z+b)} \theta^{(z+a)-1}(1-\theta)^{(n-z+b)-1} \cdot \theta^{T}(1-\theta)^{1-T}  d\theta\\
\end{align}
$$




### Pergunta 4

Explique precisamente o significado do principio da verossimilhanca. Mostre um exemplo da inferencia classica onde ele é violado.


