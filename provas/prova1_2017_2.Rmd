---
title: "Prova 1-2017/2"
author: "Davi Wentrick Feijó"
date: "2023-10-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Pergunta 1

Seja uma amostra \(X\) da distribuição exponencial com parâmetro \(\theta\) e densidade \(p(x|\theta) = \theta e^{-\theta x}\) (\(x, \theta > 0\)) e considere uma distribuição a priori \(\theta \sim \text{Gama}(\alpha, \beta)\).

#### (a) Calcule a distribuição a posteriori \(p(\theta | X)\).

Seja $X = (X_1,...,X_n)$

Sabemos que para calcular a posteriori podemos usar a seguinte formula:


$$
p(\theta | X) = \frac{p(X | \theta) \cdot p(\theta)}{p(X)} 
$$

Sabemos que :

+ $p(X | \theta)$ é a nossa verossimilhanca 

+ $p(\theta)$ é a priori

+ $p(X)$ é a constante de normalizacao 

Entretanto a posteriori pode ser expressa da seguinte forma:

$$
p(\theta | X)  \varpropto  p(X | \theta) \cdot p(\theta) 
$$

Isso significa que a distribuicao a posteriori vai ser proporcional a $p(X | \theta) \cdot p(\theta)$. O que faz sentido tendo em vista que tiramos a constante de normalizacao, logo nao vao estar na mesma escala e por isso serao proporcionais.

O problema nos da:

+ $p(X | \theta)$   = $\theta e^{-\theta x}$

+ $p(\theta)$ = $\text{Gama}(\alpha, \beta)$ = $\frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{\alpha - 1} e^{-\beta \theta}$


Vamos encontrar $p(\theta | X)$ nossa verossimilhanca:

$$
p(X | \theta) = \prod_{i=1}^{n}(\theta e^{-\theta x_i}) = \theta^n e^{-\theta n \bar X}
$$
Logo podemos ir para o calculo da posteriori:

$$

\begin{align}

p(\theta | X)  &\varpropto  p(X | \theta) \cdot p(\theta) \\

&\varpropto  \theta^n e^{-\theta n \bar X} \cdot \text{Gama}(\alpha, \beta) \\

&\varpropto  \theta^n e^{-\theta n \bar X} \cdot \frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{\alpha - 1} e^{-\beta \theta} \\

&\varpropto  \theta^n e^{-\theta n \bar X} \cdot  \theta^{\alpha - 1} e^{-\beta \theta} \\

&\varpropto  \theta^{(n+\alpha)-1} e^{-(\beta+ n \bar X) \theta} \sim Gama(n+\alpha,\beta+ n \bar X) \\

\end{align}

$$

Obs: Foi retirado os termos constantes que nao dependem de \theta devido ao fato de estarmos buscando uma distribuicao proporcional, esses termos seriam para normalizacao da distribuicao econtrada. Ao final da conta foi feita manipulacao nos expoentes para que fique no mesmo formato de uma distribuicao Gama


#### (b) Calcule o valor esperado a posteriori de \(\theta\) e mostre que ele pode ser escrito da forma \(wE(\theta) + (1 - w)\hat{\theta}\), onde \(E(\theta)\) é o valor esperado a priori e \(\hat{\theta}\) é o estimador de máxima verossimilhança.

Sabemos que a distribuicao Gama tem esperanca na forma : $\frac{\alpha}{\beta}$, logo o valor esperado a posteriori será:

$$
E(\theta)_{post} = \frac{n+\alpha}{\beta+ n \bar X}
$$

Para a realizacao do calculo vamos tomar a reciproca da $E(\theta)_{post}$:

$$
\begin{align}

E(\theta)_{post}^{-1} &= \frac{ n \bar X+\beta}{\alpha+n} \\

 &= \frac{ n \bar X+\beta}{\alpha+n} \\
 &= \frac{ \sum_{i=1}^{n}x_i+\beta}{\alpha+n} \\
 &= \frac{ \sum_{i=1}^{n}x_i}{\alpha+n} \cdot \frac{n}{n} + \frac{\beta}{\alpha+n} \cdot \frac{\alpha}{\alpha} \\
 &= \frac{n \sum_{i=1}^{n}x_i }{(\alpha+n)n}  + \frac{\beta \alpha}{(\alpha+n) \alpha} \\
 &= \frac{ n}{\alpha+n} \cdot \frac{\sum_{i=1}^{n}x_i}{n} + \frac{\alpha}{\alpha+n} \cdot \frac{\beta}{\alpha} \\
\end{align}
$$

Apos as manipulacoes algebricas, chegamos numa forma que é similar a forma desejada $wE(\theta) + (1 - w)\hat{\theta}$:

+ $E(\theta)$ = $\frac{\sum_{i=1}^{n}x_i}{n}$

+ $\hat{\theta}$ = $\frac{\beta}{\alpha}$

Nosso $w$ será:

$$
w = \frac{n}{\alpha+n} 
$$
E a partir disso podemos calcular $1-w$

$$
 1-w = 1 - \frac{n}{\alpha+n} = \frac{\alpha+n-n}{\alpha+n} = \frac{\alpha}{\alpha+n}
$$

Mostramos que os resultados batem com o esperado, logo a equacao pode ser reescrita da seguinte forma:

+ $E(\theta)$ = $\frac{\sum_{i=1}^{n}x_i}{n}$

+ $\hat{\theta}$ = $\frac{\beta}{\alpha}$

+ $W$ = $\frac{n}{\alpha+n}$

+ $1-W$ = $\frac{\alpha}{\alpha+n}$

$$
\frac{ n}{\alpha+n} \cdot \frac{\sum_{i=1}^{n}x_i}{n} + \frac{\alpha}{\alpha+n} \cdot \frac{\beta}{\alpha} = wE(\theta) + (1 - w)\hat{\theta}
$$


#### (c) Se \(X_{n+1}\) é uma observação futura da mesma distribuição exponencial, calcule a distribuição preditiva \(p(x_{n+1} | X)\) e o valor esperado \(E(X_{n+1} | X)\).

Sabemos que:

$$
p(\theta | X) \varpropto \theta^{(n+\alpha)-1} e^{-(\beta+ n \bar X) \theta} \sim Gama(n+\alpha,\beta+ n \bar X)
$$

Os hiperparametros da distribuicao a posteriori é:

$$
\alpha^{\ast} = n+\alpha \ \ \  \ \ \ \beta^{\ast} = \beta+ n \bar X \ \ \ \sim Gama(\alpha^{\ast},\beta^{\ast}) 
$$
A verossimilhanca preditiva será:

$$
p(X | \theta) = \prod_{i=1}^{n+1}(\theta e^{-\theta x_i}) = \theta^{n+1} e^{-\theta \sum_{i=1}^{n+1} x_i} = \theta^{n+1} e^{-\theta (n+1) \bar X}
$$




Nossa distribuicao preditiva vai ser dado por:

$$
\begin{align}
p(x_{n+1}|X) = \int p(x_{n+1}, \theta|X) d\theta &= \int p(x_{n+1}| X,\theta) p(\theta|X) d\theta \\
 &= \int p(x_{n+1}| X,\theta) p(\theta|X) d\theta \\
 &= \int \theta^{n+1} e^{-\theta (n+1) \bar X} \cdot \theta^{(n+\alpha)-1} e^{-(\beta+ n \bar X) \theta} d\theta \\
 &= \int \theta^{n+1}  \cdot \theta^{(n+\alpha)-1} \cdot e^{-\theta (n+1) \bar X} \cdot  e^{-(\beta+ n \bar X) \theta} d\theta \\
 &= \int \theta^{(n+\alpha)-1+(n+1)} \cdot e^{-(\beta+ n \bar X) \theta + (-\theta (n+1) \bar X)} d\theta \\
 &= \int \theta^{n+\alpha-1+n+1)} \cdot e^{-(\beta+ n \bar X) \theta -\theta (n+1) \bar X} d\theta \\
 &= \int \theta^{2n+\alpha} \cdot e^{-\theta[(\beta+ n \bar X)+(n+1) \bar X]} d\theta \\
\end{align}
$$

### Pergunta 2

Sejam \(y_1, y_2, \ldots, y_n\) observações independentes com \(y_i \sim \text{Poisson}(\theta x_i)\), onde as exposições \(x_i\) são fixas (não aleatórias). A taxa \(\theta\) segue a priori uma distribuição \(\text{Gama}(\alpha, \beta)\).

#### (a) Ache a distribuição a posteriori de \(\theta\) e sua média e variância.

Sabemos que :

$$
y_i|\theta \sim \text{Poisson}(\theta x_i)
$$
Ou seja cada observacao segue uma poisson, entretanto para resolver precisamos encontrar a distribuicao de Y. Para isso vamos realizar o somatorio em $y_i$.

$$
Y = \sum_{i=1}^{n} y_i|\theta \sim \text{Poisson}(\theta n \bar X) = \frac{(\theta n \bar X)^{y_i}e^{-n \bar X \theta}}{y_i!} 
$$

Obs:  A substituicao feita foi com base na formula para o calculo da media onde $\sum_{i=1}^{n} x_i = n \bar X$


Agora podemos calcular a verossimilhanca:

$$
\begin{align}
p(Y|n \bar X) &= \prod_{i=1}^{n} \frac{(\theta n \bar X)^{y_i}e^{-n \bar X \theta}}{y_i!}  \\
 &=  \frac{(\theta n \bar X)^{n \bar Y}e^{-n \bar X \theta}}{y_i!} \varpropto  \theta^{n \bar Y}  e^{-n \bar X \theta}
\end{align}
$$
Foram retirados todos as partes que sao independentes de \theta por isso usamos o simbolo de proporcionalidade ja que sao termos responsaveis pela normalizacao da distribuicao (entre 0 e 1).

A nossa priori é:

$$
p(\theta) \sim Gama(\alpha,\beta) \\

p(\theta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{\alpha - 1} e^{-\beta \theta} \\
p(\theta) \varpropto  \theta^{\alpha - 1} e^{-\beta \theta}
$$

Podemos iniciar o calculo da posteriori:

$$
\begin{align}
p(\theta|Y) \varpropto p(Y|n \bar X) \cdot p(\theta) &=  \theta^{n \bar Y}  e^{-n \bar X \theta} \cdot \theta^{\alpha - 1} e^{-\beta \theta}\\
&=  \theta^{(n \bar Y+\alpha)-1}  e^{-(\beta + n \bar X) \theta} \sim Gama(n \bar Y+\alpha,\beta + n \bar X)
\end{align}
$$

Com a distribuicao calculada podemos encontrar a média e a variancia com base nas formulas da Gama, logo:

$$
E(\theta|Y) = \frac{n \bar Y+\alpha}{\beta + n \bar X} \ \ \ \ \ \  \ \  \ Var(\theta|Y) = \frac{n \bar Y+\alpha}{(\beta + n \bar X)^2}
$$


#### (b) Mostre que é possível expressar a esperança a posteriori de \(\theta\) da forma \((1 - w)E(\theta) + w\hat{\theta}\), onde \(E(\theta)\) e \(\hat{\theta}\) são, respectivamente, a esperança a priori e a estimativa de máxima verossimilhança de \(\theta\), e interprete este resultado.

Vamos encontrar o $\hat \theta$

$$
\begin{align}
\hat \theta &= \frac{\partial }{\partial \theta} log(\theta^{n \bar Y}  e^{-n \bar X \theta}) = 0 \\ 
&= \frac{\partial }{\partial \theta} n \bar Ylog(\theta)-n \bar X \theta = 0 \\
&=  \frac{n \bar Y}{\hat\theta}-n \bar X  = 0 \\
&=  n[\frac{ \bar Y}{\hat\theta}- \bar X]  = 0 \\
&=  \frac{ \bar Y}{\hat\theta}- \bar X  = \frac{0}{n} \\
&=  \frac{ \bar Y}{\hat\theta}- \bar X  = 0 \\
&=  \frac{ \bar Y}{\hat\theta}   = \bar X \\
&=  \frac{ \bar Y}{\bar X}   =  \hat\theta\\
\end{align}
$$
$E(\theta)$ é a nossa esperanca a priora ou seja é a esperanca que vem da distribuicao $Gama(\alpha,\beta)$

$$
E(\theta) = \frac{\alpha}{\beta}
$$


Com isso podemos ir para a prova:

$$
\begin{align}
E(\theta|Y) &=  \frac{n \bar Y+\alpha}{\beta + n \bar X} \\
&=  \frac{n \bar Y}{\beta + n \bar X} + \frac{\alpha}{\beta + n \bar X}\\
&=  \frac{n \bar Y}{\beta + n \bar X} \cdot \frac{\bar X}{\bar X} + \frac{\alpha}{\beta + n \bar X} \cdot \frac{\beta}{\beta} \\
&=  \frac{n \bar X}{\beta + n \bar X} \cdot \frac{\bar Y}{\bar X} + \frac{\beta}{\beta + n \bar X} \cdot \frac{\alpha}{\beta} \\
\end{align}
$$

Agora que manipulamos a equacao para ficar no formato desejado temos:

+ $w = \frac{n \bar X}{\beta + n \bar X}$

+ $1 - w = \frac{\beta}{\beta + n \bar X}$

+ $E(\theta) = \frac{\alpha}{\beta}$

+ $\hat\theta = \frac{ \bar Y}{\bar X}$


#### (c) O que acontece na parte (b) quando \(\beta\) é grande com \(\alpha\beta^{-1}\) fixo? Interprete!



### Pergunta 3

Seja \(y_1, y_2, \ldots, y_n\) uma amostra da distribuição de Bernoulli com probabilidade de sucesso \(\theta\) e considere uma distribuição a priori uniforme para \(\theta\).

#### (a) Ache a distribuição a posteriori de \(\theta\) e sua média e variância.



#### (b) Mostre que é possível expressar a esperança a posteriori de \(\theta\) da forma \((1 - w)E(\theta) + w\hat{\theta}\), onde \(E(\theta)\) e \(\hat{\theta}\) são, respectivamente, a esperança a priori e a estimativa de máxima verossimilhança de \(\theta\), e interprete este resultado.



#### (c) Se \(y_{n+1}\) é uma observação futura deste processo de Bernoulli, ache a distribuição preditiva \(p(y_{n+1}|y_1, \ldots, y_n)\).

### Pergunta 4

Explique precisamente o significado do principio da verossimilhanca. Mostre um exemplo da inferencia classica onde ele é violado.


